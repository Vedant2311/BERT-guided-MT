{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfa85f2-147a-4c51-9741-d72851dff8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from transformers.optimization import AdamW\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98d26d9f-4f60-4dc6-a61d-9a2e14c467da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):                                  \n",
    "\n",
    "    def __init__(self, ne_file, en_file, max_length, tokenizer):\n",
    "        self.ne_file = ne_file\n",
    "        self.en_file = en_file\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ne_token = []\n",
    "        self.en_token = []\n",
    "        self.en = []\n",
    "        self.ne = []\n",
    "        with open(self.ne_file) as file:\n",
    "            for sentence in file:\n",
    "                # is padding token 1?\n",
    "                sentence = sentence.strip()\n",
    "                tokens = tokenizer(sentence, max_length=self.max_length, return_tensors=\"pt\", truncation=True, padding='max_length')\n",
    "                self.ne.append(sentence)\n",
    "                self.ne_token.append(tokens)\n",
    "        with open(self.en_file) as file_en:\n",
    "            for sentence in file_en:\n",
    "                # is padding token 1?\n",
    "                sentence = sentence.strip()\n",
    "                tokens = tokenizer(sentence, max_length=self.max_length, return_tensors=\"pt\", truncation=True, padding='max_length')\n",
    "                self.en.append(sentence)\n",
    "                self.en_token.append(tokens)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.ne_token)\n",
    "              \n",
    "    # input_ids attention_mask encoder_mask decoder_mask \n",
    "    # come back and fix shitty [0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'ne_tokens': self.ne_token[idx]['input_ids'][0],\n",
    "            'ne_mask': self.ne_token[idx]['attention_mask'][0],\n",
    "            'en_tokens': self.en_token[idx]['input_ids'][0],\n",
    "            'en_mask': self.en_token[idx]['attention_mask'][0],\n",
    "            'en': self.en[idx],\n",
    "            'ne': self.ne[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7759508-5c98-4469-a829-680260d64cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasets\n",
    "DIR_PATH = \"/workspace\"\n",
    "BATCH_SIZE = 32\n",
    "# A flag to see whether we are fine-tuning the model or not\n",
    "fine_tune = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08feb442-037c-47a4-a9dd-b05b23840666",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init models\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('init models')\n",
    "#initlaize model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.cuda()\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "tokenizer.src_lang = \"ne_NP\"\n",
    "tokenizer.tgt_lang = \"en_XX\"\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346fe5c6-5e1d-4823-b8a5-d7d7f7398f10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepping data\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('prepping data')\n",
    "train_dataset = LanguageDataset(f'{DIR_PATH}/train.ne_NP.txt', f'{DIR_PATH}/train.en_XX.txt', BATCH_SIZE, tokenizer)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1d53e5f-2a81-41d3-875e-c426731ab114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine tuning\n",
      "epoch 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 5114/5114 [32:08<00:00,  2.65batch/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Perform fine-tuning of mBART in case the flag is set appropriately\n",
    "You can check out references like these to understand the code better:\n",
    "    - https://colab.research.google.com/drive/1d2mSWLiv93X2hgt9WV8IJFms9dQ6arLn?usp=sharing\n",
    "    - https://github.com/huggingface/transformers/issues/23185#issuecomment-1537690520\n",
    "'''\n",
    "if fine_tune:\n",
    "    print('fine tuning')\n",
    "    # Moving the model to CUDA\n",
    "    model = model.cuda()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    model.train()\n",
    "        \n",
    "    num_epochs = 1\n",
    "    # Fine-tune for the specified number of epochs\n",
    "    for i in range(num_epochs):\n",
    "        print(f'epoch {i+1} of {num_epochs}')\n",
    "        total_batches = len(train_loader)\n",
    "        counter = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, total=total_batches, desc=\"Training\", unit=\"batch\"):\n",
    "            counter += 1\n",
    "            model_inputs = {\n",
    "                'input_ids': batch['ne_tokens'].to('cuda'),\n",
    "                'attention_mask': batch['ne_mask'].to('cuda')\n",
    "            }\n",
    "            labels = batch['en_tokens'].to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**model_inputs, labels=labels)  # Forward pass\n",
    "            loss = output.loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a415b86-caf7-410c-aa17-cad1bfa23b66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score:  0.14348966088442014\n"
     ]
    }
   ],
   "source": [
    "test_dataset = LanguageDataset(f'{DIR_PATH}/test.ne_NP.txt', f'{DIR_PATH}/test.en_XX.txt', BATCH_SIZE, tokenizer)\n",
    "\n",
    "#init generated translations\n",
    "generated_translations = []\n",
    "reference = []\n",
    "\n",
    "#for each nepali sentence, generate english translation, and add to generated translations\n",
    "for sentence in test_dataset:\n",
    "\n",
    "    input_ids = tokenizer(sentence['ne'], return_tensors=\"pt\").input_ids.to('cuda')\n",
    "    forced_bos_token_id = tokenizer.lang_code_to_id[\"ne_NP\"]\n",
    "    # input_ids = sentence['en_tokens'].to('cuda').reshape(1, -1)\n",
    "    outputs = model.generate(input_ids=input_ids, forced_bos_token_id=forced_bos_token_id, max_length=BATCH_SIZE)\n",
    "\n",
    "    english_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    generated_translations.append(english_translation)\n",
    "\n",
    "    reference.append(sentence['en'])\n",
    "    \n",
    "hypotheses = [gen.split() for gen in generated_translations]\n",
    "references = [[ref.split()] for ref in reference]\n",
    "\n",
    "print(\"bleu score: \", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "daabbbc7-a384-4d30-89e5-4a0af2f9fe83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score:  0.14348966088442014\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "#get nepali sentences and english references\n",
    "dataset_np = load_dataset(\"text\", data_files= {\"train\": f\"{DIR_PATH}/train.ne_NP.txt\", \"test\": f\"{DIR_PATH}/test.ne_NP.txt\"})\n",
    "dataset_en = load_dataset(\"text\", data_files={\"train\": f\"{DIR_PATH}/train.en_XX.txt\", \"test\": f\"{DIR_PATH}/test.en_XX.txt\"})\n",
    "nepali_sentences = dataset_np[\"test\"]\n",
    "english_references = dataset_en[\"test\"]\n",
    "\n",
    "#init generated translations\n",
    "generated_translations = []\n",
    "\n",
    "#for each nepali sentence, generate english translation, and add to generated translations\n",
    "for nepali_sentence in nepali_sentences:\n",
    "\n",
    "    input_ids = tokenizer(nepali_sentence['text'], return_tensors=\"pt\").input_ids.to('cuda')\n",
    "    \n",
    "    #find forced beginning of sentence token id\n",
    "    forced_bos_token_id = tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "\n",
    "    outputs = model.generate(input_ids=input_ids, forced_bos_token_id=forced_bos_token_id, max_length=BATCH_SIZE)\n",
    "\n",
    "    #decode generated english translation back into a sentence\n",
    "    english_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # add english sentence to generated translations\n",
    "    generated_translations.append(english_translation)\n",
    "\n",
    "#calculate bleu score\n",
    "# map each sentence to a [ sentence.split() ]\n",
    "\n",
    "references = [[reference.split()] for reference in english_references[\"text\"]]\n",
    "hypotheses = [gen.split() for gen in generated_translations]\n",
    "\n",
    "bleu_score = corpus_bleu(references, hypotheses)\n",
    "print(\"bleu score: \", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e923b1-bbe1-42bb-a083-7ede797561c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
