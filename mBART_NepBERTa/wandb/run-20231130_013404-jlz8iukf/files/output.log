All TF 2.0 model weights were used when initializing BertForMaskedLM.
Some weights of BertForMaskedLM were not initialized from the TF 2.0 model and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training Loop:   0%|                                                                                                                                                                       | 0/20 [00:00<?, ?it/s]
Batch Splitting Loop:   0%|                                                                                                                                                             | 0/20454 [00:00<?, ?it/s]

  warnings.warn('Was asked to gather along dimension 0, but all '                                                                                                                       | 0/20454 [00:00<?, ?it/s]



























































































































































Training Loop:   0%|                                                                                                                                                                       | 0/20 [05:16<?, ?it/s]
Traceback (most recent call last):
  File "/scratch1/yoonsoon/machine_translate/mBART_NepBERTa/finetuneNBmB.py", line 157, in <module>
    main()
  File "/scratch1/yoonsoon/machine_translate/mBART_NepBERTa/finetuneNBmB.py", line 148, in main
    finetune()
  File "/scratch1/yoonsoon/machine_translate/mBART_NepBERTa/finetuneNBmB.py", line 93, in finetune
    for batch in tqdm(dataloader, desc="Batch Splitting Loop"):
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/tqdm/std.py", line 1178, in __iter__
    for obj in iterable:
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 678, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/scratch1/yoonsoon/machine_translate/mBART_NepBERTa/customDataset.py", line 78, in __getitem__
    mbart_input_ids, labels, mbart_input_ids_len = self.tokenize_mbart_label(src_text, trg_text, self.mbart_tokenizer)
  File "/scratch1/yoonsoon/machine_translate/mBART_NepBERTa/customDataset.py", line 36, in tokenize_mbart_label
    inputs = tokenizer(src, text_target=trg)
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2798, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2904, in _call_one
    return self.encode_plus(
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2977, in encode_plus
    return self._encode_plus(
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py", line 576, in _encode_plus
    batched_output = self._batch_encode_plus(
  File "/home1/yoonsoon/.conda/envs/startenv/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py", line 504, in _batch_encode_plus
    encodings = self._tokenizer.encode_batch(
KeyboardInterrupt